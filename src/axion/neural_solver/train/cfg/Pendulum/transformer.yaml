env:
  env_name: Pendulum
  num_envs: 2
  neural_integrator_cfg: 
    name: TransformerNeuralIntegrator
    states_frame: body
    anchor_frame_step: every
    states_embedding_type: identical
    prediction_type: relative
    num_states_history: 10     # number of states and actions to pass into model. Default: 1 means use current state and action
  
algorithm:
  name: SequenceModelTrainer
  num_epochs: 5
  num_iters_per_epoch: 100
  sample_sequence_length: 10 # the length to sample sequence from dataset
  batch_size: 512
  num_valid_batches: 10
  truncate_grad: True
  grad_norm: 1.0
  optimizer:
    lr_start: 1e-3
    lr_end: 1e-4
    lr_schedule: linear

  dataset:
    train_dataset_path: src/axion/neural_solver/datasets/Pendulum/pendulumStatesOnly500TrainSeed0.hdf5
    valid_datasets: 
      valid: src/axion/neural_solver/datasets/Pendulum/pendulumStatesOnly500ValidSeed1.hdf5
      #exp_trajectory: PATH/TO/VALID_DATASET_1.hdf5
      #fixed_ground_passive: PATH/TO/VALID_DATASET_2.hdf5
    max_capacity: 100000000
    num_data_workers: 4
  
  eval:
    mode: dataset
    rollout_horizon: 10
    num_rollouts: 32
    dataset_path: src/axion/neural_solver/datasets/Pendulum/pendulumStatesOnly2kValidSeed1.hdf5
    passive: True

inputs:
  low_dim: [
    states_embedding,
    gravity_dir
  ]
  
network:
  normalize_input: True
  normalize_output: True
  
  encoder:
    low_dim:
      layer_sizes: []
      activation: relu
      layernorm: False

  transformer:
    block_size: 16
    n_layer: 6       # number of transformer blocks
    n_head: 12        # num transformer heads
    n_embd: 192       # size of input embedding
    dropout: 0.0      # for pretraining 0 is good, for finetuning try 0.1+
    bias: False       # do we use bias inside LayerNorm and Linear layers?

  model:
    output_tanh: False
    mlp: 
      layer_sizes: [64]
      activation: relu
      layernorm: False